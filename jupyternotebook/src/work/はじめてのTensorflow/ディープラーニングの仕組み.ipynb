{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ディープラーニングの仕組み\n",
    "ディープラーニングはそれまでのニューラルネットワークに比べて多くの中間層を持つことができ、より複雑な問題を解くことができます。よって、ディープニューラルネットワーク（DeepNeuralNetwork：DNN）とも呼びます。ディープラーニングは画像や音声、テキストデータなど次元数が多い非構造化データの扱いが得意です。中間層を増やし、層を深くしていくと、先に説明した計算方法だけでは学習を適切に行うことができません（勾配消失問題）。しかし事前学習を行えば、層を深くしても学習を適切に行うことができます。この事前学習が、ニューラルネットワークからディープラーニングへと進化する契機となりました。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## オートエンコーダ\n",
    "\n",
    "オートエンコーダは、出力データを入力データに近付けるように（つまりは自分自身を再現できるように）学習する手法です。いわば自分自身が正解データとなり、正解データを別途用意する必要がないことから、教師なし学習に分類されます。事前学習を行う目的は、複雑な問題を解けるようにするために、中間層の数を増やしたとき、学習が適切に行われるようにすることにあります。このとき、入力層側のエッジの重みは、入力層から伝わるデータを圧縮して上手く特徴量を抽出できるように調整されます。出力層側のエッジの重みは、特徴量から元のデータを復元できるように調整されます。このようにして中間層を1層ずつ追加していき、深いネットワークを構築します。事前学習によってネットワークを構築できたら、今度は正解データを用いて教師あり学習を行い、学習モデルの精度を高めていきます。2012年にGoogle社が発表して話題となった、猫を認識したAIにも、オートエンコーダが用いられていました。人間が機械に「これ（対象）は猫だ」と正解を教えることなく、機械が自分で学習し、対象が猫だと理解したというものです。厳密には「猫の認識」ではなく、「猫を表現する特徴量を得ることができた」とするのが正しい表現です(注1)。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
